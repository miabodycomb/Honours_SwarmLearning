{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOfbThY9QPASMpcfOw6u62D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LWhVdMYWiPhe"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from torchvision import datasets, transforms\n","import sklearn\n","from torch.utils.data import DataLoader, random_split, ConcatDataset, Subset\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import os\n","from google.colab import drive\n","import concurrent.futures\n","import time\n","import random\n","import csv"]},{"cell_type":"code","source":["# Mount Google Drive for persistent storage\n","drive.mount('/content/drive')"],"metadata":{"id":"7kqCkRdibcHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_NODES = 4\n","NUM_GLOBAL_EPOCHS = 5\n","NUM_LOCAL_EPOCHS = 5\n","train_size = 0.9\n","test_size = 0.1\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"yTUlgE767jNL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Define a simple neural network model\n","class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28 * 28)  # Flatten the input\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Client class for federated learning\n","class Client:\n","    def __init__(self, data):\n","        self.data = data\n","        self.model = SimpleNN()\n","\n","    def train(self, num_epochs=2):\n","        data_loader = DataLoader(self.data, batch_size=32, shuffle=True)\n","        optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        self.model.train()\n","        for epoch in range(num_epochs):\n","            for images, labels in data_loader:\n","                optimizer.zero_grad()\n","                outputs = self.model(images)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","    def get_weights(self):\n","        return self.model.state_dict()\n","\n","    def set_weights(self, weights):\n","        self.model.load_state_dict(weights)\n","\n","    def evaluate(self, test_loader):\n","        self.model.eval()\n","        y_true, y_pred = [], []\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                outputs = self.model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                y_true.extend(labels.cpu().numpy())\n","                y_pred.extend(predicted.cpu().numpy())\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_true, y_pred)\n","        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n","        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n","        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n","\n","        return accuracy, precision, recall, f1\n","\n","# Simulate federated learning\n","def federated_learning(num_clients=5, num_epochs=5, global_rounds=5):\n","    # Load MNIST dataset\n","    transform = transforms.Compose([transforms.ToTensor()])\n","    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","    test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n","\n","    # Total number of data points in the dataset\n","    total_size = len(dataset)\n","\n","    # Ensure each client gets at least one data point\n","    remaining_size = total_size - num_clients  # Total size minus one datapoint for each client\n","\n","    # Generate random sizes for remaining data, ensuring at least 1 datapoint per client\n","    random_sizes = [random.randint(0, remaining_size // num_clients) for _ in range(num_clients - 1)]\n","    random_sizes.append(remaining_size - sum(random_sizes))  # Adjust the last split to ensure total size matches\n","\n","    # Add 1 to each random size to ensure each client gets at least one data point\n","    random_sizes = [size + 1 for size in random_sizes]\n","\n","    # Split dataset into random sizes based on the generated sizes\n","    client_datasets = random_split(dataset, random_sizes)\n","\n","    # Create client instances\n","    clients = [Client(data) for data in client_datasets]\n","\n","    # Initialize global model\n","    global_model = SimpleNN()\n","\n","    # Prepare to store metrics\n","    metrics_list = []\n","\n","    for round in range(global_rounds):\n","        print(f\"\\nGlobal Round {round + 1}/{global_rounds}\")\n","\n","        # Local training for each client\n","        for client in clients:\n","            client.set_weights(global_model.state_dict())  # Load global model weights\n","            client.train(num_epochs=num_epochs)  # Train locally\n","\n","        # Aggregate local weights to update global model\n","        global_weights = global_model.state_dict()\n","        for key in global_weights.keys():\n","            global_weights[key] = torch.mean(torch.stack([client.get_weights()[key] for client in clients]), dim=0)\n","        global_model.load_state_dict(global_weights)\n","\n","\n","    # Write metrics to CSV\n","            # Evaluate global model\n","    accuracy, precision, recall, f1 = evaluate_global_model(global_model, test_loader)\n","    metrics_list.append({\n","        'Round': round + 1,\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1\n","    })\n","    metrics_df = pd.DataFrame(metrics_list)\n","    # Append the metrics to the CSV file, adding a new line\n","    metrics_df.to_csv('/content/drive/My Drive/Swarm_Learning/federated_learning_MNIST.csv', mode='a', header=False, index=False)\n","    print(\"Metrics saved to 'federated_learning_metrics.csv'.\")\n","\n","def evaluate_global_model(model, test_loader):\n","    model.eval()\n","    y_true, y_pred = [], []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n","    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n","    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n","\n","    return accuracy, precision, recall, f1\n"],"metadata":{"id":"5TNUYGvn7kud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the federated learning simulation\n","for i in range(30):\n","  federated_learning(num_clients=5, num_epochs=5, global_rounds=5)"],"metadata":{"id":"m3QDUXPs7nSE"},"execution_count":null,"outputs":[]}]}